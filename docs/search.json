[
  {
    "objectID": "hw9_code.html",
    "href": "hw9_code.html",
    "title": "More Modeling",
    "section": "",
    "text": "library(tidyverse)\nlibrary(lubridate)\nlibrary(tidymodels)\nlibrary(workflows)\nlibrary(glmnet)\nlibrary(tree)\nlibrary(rpart)\nlibrary(rpart.plot)\nlibrary(baguette)\nlibrary(ranger)"
  },
  {
    "objectID": "hw9_code.html#reading-in-data",
    "href": "hw9_code.html#reading-in-data",
    "title": "More Modeling",
    "section": "Reading in Data",
    "text": "Reading in Data\nHere I am using encoding = “latin1” to deal with the file that includes non-ASCII characters such as “Temperature(캜)”.\n\nbike_data &lt;- readr::read_csv(\"SeoulBikeData.csv\", locale = readr::locale(encoding = \"latin1\"))\n\nRows: 8760 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): Date, Seasons, Holiday, Functioning Day\ndbl (10): Rented Bike Count, Hour, Temperature(°C), Humidity(%), Wind speed ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nAdjusting the date format and inspecting column values\n\nbike_data &lt;- bike_data |&gt;\nmutate(date = lubridate::dmy(Date)) |&gt;\nselect(-Date)\n\nsummary(bike_data)\n\n Rented Bike Count      Hour       Temperature(°C)   Humidity(%)   \n Min.   :   0.0    Min.   : 0.00   Min.   :-17.80   Min.   : 0.00  \n 1st Qu.: 191.0    1st Qu.: 5.75   1st Qu.:  3.50   1st Qu.:42.00  \n Median : 504.5    Median :11.50   Median : 13.70   Median :57.00  \n Mean   : 704.6    Mean   :11.50   Mean   : 12.88   Mean   :58.23  \n 3rd Qu.:1065.2    3rd Qu.:17.25   3rd Qu.: 22.50   3rd Qu.:74.00  \n Max.   :3556.0    Max.   :23.00   Max.   : 39.40   Max.   :98.00  \n Wind speed (m/s) Visibility (10m) Dew point temperature(°C)\n Min.   :0.000    Min.   :  27     Min.   :-30.600          \n 1st Qu.:0.900    1st Qu.: 940     1st Qu.: -4.700          \n Median :1.500    Median :1698     Median :  5.100          \n Mean   :1.725    Mean   :1437     Mean   :  4.074          \n 3rd Qu.:2.300    3rd Qu.:2000     3rd Qu.: 14.800          \n Max.   :7.400    Max.   :2000     Max.   : 27.200          \n Solar Radiation (MJ/m2)  Rainfall(mm)     Snowfall (cm)       Seasons         \n Min.   :0.0000          Min.   : 0.0000   Min.   :0.00000   Length:8760       \n 1st Qu.:0.0000          1st Qu.: 0.0000   1st Qu.:0.00000   Class :character  \n Median :0.0100          Median : 0.0000   Median :0.00000   Mode  :character  \n Mean   :0.5691          Mean   : 0.1487   Mean   :0.07507                     \n 3rd Qu.:0.9300          3rd Qu.: 0.0000   3rd Qu.:0.00000                     \n Max.   :3.5200          Max.   :35.0000   Max.   :8.80000                     \n   Holiday          Functioning Day         date           \n Length:8760        Length:8760        Min.   :2017-12-01  \n Class :character   Class :character   1st Qu.:2018-03-02  \n Mode  :character   Mode  :character   Median :2018-06-01  \n                                       Mean   :2018-06-01  \n                                       3rd Qu.:2018-08-31  \n                                       Max.   :2018-11-30  \n\n\nTurning variables into factors\n\nbike_data &lt;- bike_data |&gt;\nmutate(seasons = factor(Seasons),\nholiday = factor(Holiday),\nfn_day = factor(`Functioning Day`)) |&gt;\nselect(-Seasons, -Holiday, -`Functioning Day`)\n\nRenaming variables\n\nbike_data &lt;- bike_data |&gt;\nrename('bike_count' = `Rented Bike Count`,\n'hour' = \"Hour\",\n\"temp\" = `Temperature(°C)`,\n\"wind_speed\" = `Wind speed (m/s)`,\n\"humidity\" = `Humidity(%)`,\n\"vis\" = `Visibility (10m)`,\n\"dew_point_temp\" = `Dew point temperature(°C)`,\n\"solar_radiation\" = `Solar Radiation (MJ/m2)`,\n\"rainfall\" = \"Rainfall(mm)\",\n\"snowfall\" = `Snowfall (cm)`)\n\nRemoving days where they aren’t in operation\n\nbike_data &lt;- bike_data |&gt;\nfilter(fn_day == \"Yes\") |&gt;\nselect(-fn_day)\n\nSummarizing across the hours so each day has one observation\n\nbike_data &lt;- bike_data |&gt;\n  group_by(date, seasons, holiday) |&gt;\n  summarize(bike_count = sum(bike_count),\n            temp = mean(temp),\n            humidity = mean(humidity),\n            wind_speed = mean(wind_speed),\n            vis = mean(vis),\n            dew_point_temp = mean(dew_point_temp),\n            solar_radiation = mean(solar_radiation),\n            rainfall = sum(rainfall),\n            snowfall = sum(snowfall)) |&gt;\n  ungroup()\n\n`summarise()` has grouped output by 'date', 'seasons'. You can override using\nthe `.groups` argument.\n\nbike_data\n\n# A tibble: 353 × 12\n   date       seasons holiday    bike_count    temp humidity wind_speed   vis\n   &lt;date&gt;     &lt;fct&gt;   &lt;fct&gt;           &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;\n 1 2017-12-01 Winter  No Holiday       9539 -2.45       45.9      1.54  1871.\n 2 2017-12-02 Winter  No Holiday       8523  1.32       62.0      1.71  1471.\n 3 2017-12-03 Winter  No Holiday       7222  4.88       81.5      1.61   456.\n 4 2017-12-04 Winter  No Holiday       8729 -0.304      52.5      3.45  1363.\n 5 2017-12-05 Winter  No Holiday       8307 -4.46       36.4      1.11  1959.\n 6 2017-12-06 Winter  No Holiday       6669  0.0458     70.8      0.696 1187.\n 7 2017-12-07 Winter  No Holiday       8549  1.09       67.5      1.69   949.\n 8 2017-12-08 Winter  No Holiday       8032 -3.82       41.8      1.85  1872.\n 9 2017-12-09 Winter  No Holiday       7233 -0.846      46        1.08  1861.\n10 2017-12-10 Winter  No Holiday       3453  1.19       69.7      2.00  1043.\n# ℹ 343 more rows\n# ℹ 4 more variables: dew_point_temp &lt;dbl&gt;, solar_radiation &lt;dbl&gt;,\n#   rainfall &lt;dbl&gt;, snowfall &lt;dbl&gt;"
  },
  {
    "objectID": "hw9_code.html#eda",
    "href": "hw9_code.html#eda",
    "title": "More Modeling",
    "section": "EDA",
    "text": "EDA\nChecking for missing values, there does not appear to be any.\n\nsum_na &lt;- function(column){\n sum(is.na(column))\n}\nna_counts &lt;- bike_data |&gt;\n summarize(across(everything(), sum_na))\n\nna_counts\n\n# A tibble: 1 × 12\n   date seasons holiday bike_count  temp humidity wind_speed   vis\n  &lt;int&gt;   &lt;int&gt;   &lt;int&gt;      &lt;int&gt; &lt;int&gt;    &lt;int&gt;      &lt;int&gt; &lt;int&gt;\n1     0       0       0          0     0        0          0     0\n# ℹ 4 more variables: dew_point_temp &lt;int&gt;, solar_radiation &lt;int&gt;,\n#   rainfall &lt;int&gt;, snowfall &lt;int&gt;\n\n\nBasic summary stats for numeric variables focusing on bike_count.\n\nbike_data |&gt;\nsummarize(across(`bike_count`,.fns = c(\"mean\" = mean,\n                                       \"median\" = median,\n                                       \"sd\" = sd,\n                                       \"IQR\" = IQR,\n                                       \"min\" = min,\n                                       \"max\" = max),\n.names = \"{.col}_{.fn}\"))\n\n# A tibble: 1 × 6\n  bike_count_mean bike_count_median bike_count_sd bike_count_IQR bike_count_min\n            &lt;dbl&gt;             &lt;dbl&gt;         &lt;dbl&gt;          &lt;dbl&gt;          &lt;dbl&gt;\n1          17485.             18563         9937.          19318            977\n# ℹ 1 more variable: bike_count_max &lt;dbl&gt;\n\n\nLooking at different groupings.\n\nbike_data |&gt;\ngroup_by(seasons, holiday) |&gt;\nsummarize(across(`bike_count`,\n.fns = c(\"mean\" = mean,\n\"median\" = median,\n\"sd\" = sd,\n\"IQR\" = IQR,\n\"min\" = min,\n\"max\" = max),\n.names = \"{.col}_{.fn}\"))\n\n`summarise()` has grouped output by 'seasons'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 8 × 8\n# Groups:   seasons [4]\n  seasons holiday bike_count_mean bike_count_median bike_count_sd bike_count_IQR\n  &lt;fct&gt;   &lt;fct&gt;             &lt;dbl&gt;             &lt;dbl&gt;         &lt;dbl&gt;          &lt;dbl&gt;\n1 Autumn  Holiday          22754.            21705          5642.          5740 \n2 Autumn  No Hol…          22065.            23472          6792.         10734 \n3 Spring  Holiday          15247.            13790         10917.         10844 \n4 Spring  No Hol…          18002.            17730          8322.         14224.\n5 Summer  Holiday          24532.            24532.         8438.          5966.\n6 Summer  No Hol…          24824.            25572.         7324.          9165 \n7 Winter  Holiday           3759              3454.         1561.          1060.\n8 Winter  No Hol…           5574.             5609          1757.          2564 \n# ℹ 2 more variables: bike_count_min &lt;dbl&gt;, bike_count_max &lt;dbl&gt;\n\n\nExploring correlation – There are a few variables like temp and dew_point that are highly correlated.\n\nbike_data |&gt;\nselect(where(is.numeric)) |&gt;\ncor() |&gt;\nround(3)\n\n                bike_count   temp humidity wind_speed    vis dew_point_temp\nbike_count           1.000  0.753    0.036     -0.193  0.166          0.650\ntemp                 0.753  1.000    0.404     -0.261  0.002          0.963\nhumidity             0.036  0.404    1.000     -0.234 -0.559          0.632\nwind_speed          -0.193 -0.261   -0.234      1.000  0.206         -0.288\nvis                  0.166  0.002   -0.559      0.206  1.000         -0.154\ndew_point_temp       0.650  0.963    0.632     -0.288 -0.154          1.000\nsolar_radiation      0.736  0.550   -0.274      0.096  0.271          0.383\nrainfall            -0.239  0.145    0.529     -0.102 -0.222          0.265\nsnowfall            -0.265 -0.267    0.065      0.021 -0.102         -0.210\n                solar_radiation rainfall snowfall\nbike_count                0.736   -0.239   -0.265\ntemp                      0.550    0.145   -0.267\nhumidity                 -0.274    0.529    0.065\nwind_speed                0.096   -0.102    0.021\nvis                       0.271   -0.222   -0.102\ndew_point_temp            0.383    0.265   -0.210\nsolar_radiation           1.000   -0.323   -0.233\nrainfall                 -0.323    1.000   -0.023\nsnowfall                 -0.233   -0.023    1.000\n\n\nExploring plots to see relationships. It makes sense that we see the most bikes being rented in summer months.\n\nggplot(bike_data, aes(x = temp, y = bike_count)) +\ngeom_jitter(aes(color = seasons)) +\nfacet_grid(~holiday)\n\n\n\n\n\n\n\nggplot(bike_data, aes(x = solar_radiation, y = bike_count)) +\ngeom_point(aes(color = seasons)) +\nfacet_grid(~holiday)"
  },
  {
    "objectID": "hw9_code.html#splitting-the-data",
    "href": "hw9_code.html#splitting-the-data",
    "title": "More Modeling",
    "section": "Splitting the Data",
    "text": "Splitting the Data\nUsing functions from tidymodels to split the data and add in the stratification. Then applying 10-fold cross validation to the training set.\n\nset.seed(10)\nbike_split &lt;- initial_split(bike_data, prop = 0.75, strata = seasons)\nbike_train &lt;- training(bike_split)\nbike_test &lt;- testing(bike_split)\nbike_10_fold &lt;- vfold_cv(bike_train, 10)\n\nCreating the first recipe.\n\nbike_recipe1 &lt;- recipe(bike_count ~ ., data = bike_train) |&gt;\n  step_date(date, features = \"dow\") |&gt;\n  step_mutate(day_type = factor(if_else(date_dow %in% c(\"Sat\", \"Sun\"), \"Weekend\", \"Weekday\"))) |&gt;\n  step_rm(date, date_dow) |&gt;\n  step_dummy(seasons, holiday, day_type) |&gt;\n  step_normalize(all_numeric(), -bike_count) \n\nbike_recipe1\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:    1\npredictor: 11\n\n\n\n\n\n── Operations \n\n\n• Date features from: date\n\n\n• Variable mutation for: factor(if_else(date_dow %in% c(\"Sat\", \"Sun\"),\n  \"Weekend\", \"Weekday\"))\n\n\n• Variables removed: date and date_dow\n\n\n• Dummy variables from: seasons, holiday, day_type\n\n\n• Centering and scaling for: all_numeric() and -bike_count"
  },
  {
    "objectID": "hw9_code.html#fitting-mlr-model",
    "href": "hw9_code.html#fitting-mlr-model",
    "title": "More Modeling",
    "section": "Fitting MLR Model",
    "text": "Fitting MLR Model\nSetting up the linear model fit and fitting the models to determine best performance.\n\nbike_mlr &lt;- linear_reg() |&gt;\n  set_engine(\"lm\")\n\nmlr_wfl &lt;- workflow() |&gt;\n  add_recipe(bike_recipe1) |&gt;\n  add_model(bike_mlr) \n \nmlr_fit &lt;- mlr_wfl |&gt;\n   fit_resamples(bike_10_fold)\n  \n\nmlr_fit |&gt; collect_metrics()\n\n# A tibble: 2 × 6\n  .metric .estimator     mean     n  std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;         &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   4248.       10 226.     Preprocessor1_Model1\n2 rsq     standard      0.813    10   0.0197 Preprocessor1_Model1\n\nmlr_final &lt;- mlr_wfl |&gt;\n  fit(bike_train)\ntidy(mlr_final)\n\n# A tibble: 14 × 5\n   term               estimate std.error statistic   p.value\n   &lt;chr&gt;                 &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 (Intercept)          17794.      261.    68.1   4.99e-163\n 2 temp                 -2000.     4672.    -0.428 6.69e-  1\n 3 humidity             -1958.     1748.    -1.12  2.64e-  1\n 4 wind_speed            -600.      303.    -1.98  4.89e-  2\n 5 vis                   -178.      381.    -0.466 6.42e-  1\n 6 dew_point_temp        6605.     5455.     1.21  2.27e-  1\n 7 solar_radiation       4070.      491.     8.29  7.20e- 15\n 8 rainfall             -1644.      340.    -4.83  2.36e-  6\n 9 snowfall              -351.      284.    -1.24  2.17e-  1\n10 seasons_Spring       -2127.      373.    -5.70  3.35e-  8\n11 seasons_Summer       -1598.      466.    -3.43  7.02e-  4\n12 seasons_Winter       -3649.      504.    -7.24  5.41e- 12\n13 holiday_No.Holiday     638.      268.     2.38  1.80e-  2\n14 day_type_Weekend     -1190.      266.    -4.48  1.13e-  5"
  },
  {
    "objectID": "hw9_code.html#fitting-lasso-model",
    "href": "hw9_code.html#fitting-lasso-model",
    "title": "More Modeling",
    "section": "Fitting LASSO Model",
    "text": "Fitting LASSO Model\nSetting up the LASSO model and using penalty = tune() to choose the parameter\n\nbike_lasso &lt;- linear_reg(penalty = tune(), mixture = 1) |&gt;\n  set_engine(\"glmnet\")\n\nlasso_wfl &lt;- workflow() |&gt;\n  add_recipe(bike_recipe1) |&gt;\n  add_model(bike_lasso)\n\nlasso_wfl\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_date()\n• step_mutate()\n• step_rm()\n• step_dummy()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 1\n\nComputational engine: glmnet \n\n\nFitting the model with the tuning parameter.\n\nlasso_grid &lt;- lasso_wfl |&gt;\n  tune_grid(resamples = bike_10_fold,\n            grid = grid_regular(penalty(), levels = 200)) \n\nlasso_grid[1, \".metrics\"][[1]]\n\n[[1]]\n# A tibble: 400 × 5\n    penalty .metric .estimator .estimate .config               \n      &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;                 \n 1 1   e-10 rmse    standard       2776. Preprocessor1_Model001\n 2 1.12e-10 rmse    standard       2776. Preprocessor1_Model002\n 3 1.26e-10 rmse    standard       2776. Preprocessor1_Model003\n 4 1.41e-10 rmse    standard       2776. Preprocessor1_Model004\n 5 1.59e-10 rmse    standard       2776. Preprocessor1_Model005\n 6 1.78e-10 rmse    standard       2776. Preprocessor1_Model006\n 7 2.00e-10 rmse    standard       2776. Preprocessor1_Model007\n 8 2.25e-10 rmse    standard       2776. Preprocessor1_Model008\n 9 2.52e-10 rmse    standard       2776. Preprocessor1_Model009\n10 2.83e-10 rmse    standard       2776. Preprocessor1_Model010\n# ℹ 390 more rows\n\n\nExamining the rmse values and plotting them to find the best selection. From discussion post, it doesn’t seem like the flat line on the plot is an issue.\n\nlasso_grid |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"rmse\")\n\n# A tibble: 200 × 7\n    penalty .metric .estimator  mean     n std_err .config               \n      &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                 \n 1 1   e-10 rmse    standard   4245.    10    233. Preprocessor1_Model001\n 2 1.12e-10 rmse    standard   4245.    10    233. Preprocessor1_Model002\n 3 1.26e-10 rmse    standard   4245.    10    233. Preprocessor1_Model003\n 4 1.41e-10 rmse    standard   4245.    10    233. Preprocessor1_Model004\n 5 1.59e-10 rmse    standard   4245.    10    233. Preprocessor1_Model005\n 6 1.78e-10 rmse    standard   4245.    10    233. Preprocessor1_Model006\n 7 2.00e-10 rmse    standard   4245.    10    233. Preprocessor1_Model007\n 8 2.25e-10 rmse    standard   4245.    10    233. Preprocessor1_Model008\n 9 2.52e-10 rmse    standard   4245.    10    233. Preprocessor1_Model009\n10 2.83e-10 rmse    standard   4245.    10    233. Preprocessor1_Model010\n# ℹ 190 more rows\n\nlasso_grid |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"rmse\") |&gt;\n  ggplot(aes(penalty, mean, color = .metric)) +\n  geom_line()\n\n\n\n\n\n\n\n\nSelecting the best model and finalizing the workflow\n\nlowest_rmse &lt;- lasso_grid |&gt;\n  select_best(metric = \"rmse\")\nlowest_rmse\n\n# A tibble: 1 × 2\n       penalty .config               \n         &lt;dbl&gt; &lt;chr&gt;                 \n1 0.0000000001 Preprocessor1_Model001\n\nlasso_wfl |&gt;\n  finalize_workflow(lowest_rmse)\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_date()\n• step_mutate()\n• step_rm()\n• step_dummy()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = 1e-10\n  mixture = 1\n\nComputational engine: glmnet \n\n\nFitting on entire training set\n\nlasso_final &lt;- lasso_wfl |&gt;\n  finalize_workflow(lowest_rmse) |&gt;\n  fit(bike_train)\ntidy(lasso_final)\n\n# A tibble: 14 × 3\n   term               estimate      penalty\n   &lt;chr&gt;                 &lt;dbl&gt;        &lt;dbl&gt;\n 1 (Intercept)         17794.  0.0000000001\n 2 temp                   59.5 0.0000000001\n 3 humidity            -1173.  0.0000000001\n 4 wind_speed           -606.  0.0000000001\n 5 vis                  -124.  0.0000000001\n 6 dew_point_temp       4132.  0.0000000001\n 7 solar_radiation      4054.  0.0000000001\n 8 rainfall            -1691.  0.0000000001\n 9 snowfall             -367.  0.0000000001\n10 seasons_Spring      -2107.  0.0000000001\n11 seasons_Summer      -1548.  0.0000000001\n12 seasons_Winter      -3626.  0.0000000001\n13 holiday_No.Holiday    628.  0.0000000001\n14 day_type_Weekend    -1198.  0.0000000001"
  },
  {
    "objectID": "hw9_code.html#fitting-regression-tree-model",
    "href": "hw9_code.html#fitting-regression-tree-model",
    "title": "More Modeling",
    "section": "Fitting Regression Tree Model",
    "text": "Fitting Regression Tree Model\nSetting up the regression tree model and tuning parameters\n\nbike_reg &lt;- decision_tree(tree_depth = tune(),\n                           min_n = 20,\n                           cost_complexity = tune()) |&gt;\n  set_engine(\"rpart\") |&gt;\n  set_mode(\"regression\")\n\nCreating the workflow\n\nreg_wfl &lt;- workflow() |&gt;\n  add_recipe(bike_recipe1) |&gt;\n  add_model(bike_reg)\n\nExamining tuning parameters.\n\nreg_grid &lt;- grid_regular(cost_complexity(),\n                         tree_depth(),\n                         levels = c(10, 5))\n\nreg_fits &lt;- reg_wfl |&gt; \n  tune_grid(resamples = bike_10_fold,\n            grid = reg_grid)\n\nreg_fits |&gt;\n  collect_metrics()\n\n# A tibble: 100 × 8\n   cost_complexity tree_depth .metric .estimator     mean     n  std_err .config\n             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;         &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;  \n 1    0.0000000001          1 rmse    standard   6676.       10 309.     Prepro…\n 2    0.0000000001          1 rsq     standard      0.558    10   0.0463 Prepro…\n 3    0.000000001           1 rmse    standard   6676.       10 309.     Prepro…\n 4    0.000000001           1 rsq     standard      0.558    10   0.0463 Prepro…\n 5    0.00000001            1 rmse    standard   6676.       10 309.     Prepro…\n 6    0.00000001            1 rsq     standard      0.558    10   0.0463 Prepro…\n 7    0.0000001             1 rmse    standard   6676.       10 309.     Prepro…\n 8    0.0000001             1 rsq     standard      0.558    10   0.0463 Prepro…\n 9    0.000001              1 rmse    standard   6676.       10 309.     Prepro…\n10    0.000001              1 rsq     standard      0.558    10   0.0463 Prepro…\n# ℹ 90 more rows\n\n\nFinding the best value and selecting the models best tuning parameter values.\n\nreg_fits |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"rmse\") |&gt;\n  arrange( mean)\n\n# A tibble: 50 × 8\n   cost_complexity tree_depth .metric .estimator  mean     n std_err .config    \n             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;      \n 1    0.0000000001         11 rmse    standard   4020.    10    184. Preprocess…\n 2    0.000000001          11 rmse    standard   4020.    10    184. Preprocess…\n 3    0.00000001           11 rmse    standard   4020.    10    184. Preprocess…\n 4    0.0000001            11 rmse    standard   4020.    10    184. Preprocess…\n 5    0.000001             11 rmse    standard   4020.    10    184. Preprocess…\n 6    0.00001              11 rmse    standard   4020.    10    184. Preprocess…\n 7    0.0001               11 rmse    standard   4020.    10    184. Preprocess…\n 8    0.0000000001         15 rmse    standard   4020.    10    184. Preprocess…\n 9    0.000000001          15 rmse    standard   4020.    10    184. Preprocess…\n10    0.00000001           15 rmse    standard   4020.    10    184. Preprocess…\n# ℹ 40 more rows\n\nreg_best_params &lt;- select_best(reg_fits, metric = \"rmse\")\nreg_best_params\n\n# A tibble: 1 × 3\n  cost_complexity tree_depth .config              \n            &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;                \n1    0.0000000001         11 Preprocessor1_Model31"
  },
  {
    "objectID": "hw9_code.html#fitting-bagged-tree-model",
    "href": "hw9_code.html#fitting-bagged-tree-model",
    "title": "More Modeling",
    "section": "Fitting Bagged Tree Model",
    "text": "Fitting Bagged Tree Model\nSetting up the bagged tree model and tuning parameters.\n\nbike_bag &lt;- bag_tree(tree_depth = tune(),\n                           min_n = 20,\n                           cost_complexity = tune()) |&gt;\n  set_engine(\"rpart\") |&gt;\n  set_mode(\"regression\")\n\nSetting up the workflow.\n\nbag_wfl &lt;- workflow() |&gt;\n add_recipe(bike_recipe1) |&gt;\n add_model(bike_bag)\n\nFitting to cv folds\n\nbag_grid &lt;- grid_regular(cost_complexity(),\n                         tree_depth(),\n                         levels = c(10, 5))\n\nbag_fits &lt;- bag_wfl |&gt; \n  tune_grid(resamples = bike_10_fold,\n            grid = bag_grid)\n\nbag_fits |&gt;\n  collect_metrics()\n\n# A tibble: 100 × 8\n   cost_complexity tree_depth .metric .estimator     mean     n  std_err .config\n             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;         &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;  \n 1    0.0000000001          1 rmse    standard   6285.       10 262.     Prepro…\n 2    0.0000000001          1 rsq     standard      0.615    10   0.0450 Prepro…\n 3    0.000000001           1 rmse    standard   6320.       10 285.     Prepro…\n 4    0.000000001           1 rsq     standard      0.611    10   0.0472 Prepro…\n 5    0.00000001            1 rmse    standard   6171.       10 273.     Prepro…\n 6    0.00000001            1 rsq     standard      0.632    10   0.0470 Prepro…\n 7    0.0000001             1 rmse    standard   6153.       10 281.     Prepro…\n 8    0.0000001             1 rsq     standard      0.629    10   0.0451 Prepro…\n 9    0.000001              1 rmse    standard   6309.       10 253.     Prepro…\n10    0.000001              1 rsq     standard      0.611    10   0.0437 Prepro…\n# ℹ 90 more rows\n\n\nFinding the smallest rmse value and selecting the models best tuning parameter values.\n\nbag_fits |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"rmse\") |&gt;\n  arrange(mean)\n\n# A tibble: 50 × 8\n   cost_complexity tree_depth .metric .estimator  mean     n std_err .config    \n             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;      \n 1    0.00001              15 rmse    standard   3116.    10    165. Preprocess…\n 2    0.00001              11 rmse    standard   3167.    10    145. Preprocess…\n 3    0.0000000001          8 rmse    standard   3198.    10    200. Preprocess…\n 4    0.0000000001         11 rmse    standard   3226.    10    147. Preprocess…\n 5    0.001                 8 rmse    standard   3243.    10    142. Preprocess…\n 6    0.00000001           15 rmse    standard   3256.    10    126. Preprocess…\n 7    0.0001               11 rmse    standard   3257.    10    191. Preprocess…\n 8    0.0000001            15 rmse    standard   3260.    10    173. Preprocess…\n 9    0.0001               15 rmse    standard   3274.    10    158. Preprocess…\n10    0.000000001           8 rmse    standard   3278.    10    173. Preprocess…\n# ℹ 40 more rows\n\nbag_best_params &lt;- select_best(bag_fits, metric = \"rmse\")\nbag_best_params\n\n# A tibble: 1 × 3\n  cost_complexity tree_depth .config              \n            &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;                \n1         0.00001         15 Preprocessor1_Model46"
  },
  {
    "objectID": "hw9_code.html#fitting-random-forest-model",
    "href": "hw9_code.html#fitting-random-forest-model",
    "title": "More Modeling",
    "section": "Fitting Random Forest Model",
    "text": "Fitting Random Forest Model\nSetting up the bagged tree model and tuning parameters.\n\nbike_for &lt;- rand_forest(mtry = tune(),\n                        min_n = 20,)|&gt;\n  set_engine(\"ranger\", importance = \"impurity\") |&gt;\n  set_mode(\"regression\")\n\nSetting up the workflow.\n\nfor_wfl &lt;- workflow() |&gt;\n add_recipe(bike_recipe1) |&gt;\n add_model(bike_for)\n\nFitting to cv folds\n\nfor_grid &lt;- grid_regular(\n  mtry(range = c(2, 15)),    \n  levels = 10)\n\nfor_fits &lt;- for_wfl |&gt; \n  tune_grid(resamples = bike_10_fold,\n            grid = for_grid)\n\n→ A | warning: 15 columns were requested but there were 13 predictors in the data. 13 will be used.\n\n\nThere were issues with some computations   A: x1\n\n\nThere were issues with some computations   A: x2\n\n\nThere were issues with some computations   A: x5\n\n\nThere were issues with some computations   A: x8\n\n\nThere were issues with some computations   A: x10\nThere were issues with some computations   A: x10\n\n\n\n\nfor_fits |&gt;\n  collect_metrics()\n\n# A tibble: 20 × 7\n    mtry .metric .estimator     mean     n  std_err .config              \n   &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;         &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;                \n 1     2 rmse    standard   3876.       10 194.     Preprocessor1_Model01\n 2     2 rsq     standard      0.867    10   0.0160 Preprocessor1_Model01\n 3     3 rmse    standard   3605.       10 183.     Preprocessor1_Model02\n 4     3 rsq     standard      0.879    10   0.0145 Preprocessor1_Model02\n 5     4 rmse    standard   3478.       10 166.     Preprocessor1_Model03\n 6     4 rsq     standard      0.886    10   0.0129 Preprocessor1_Model03\n 7     6 rmse    standard   3366.       10 143.     Preprocessor1_Model04\n 8     6 rsq     standard      0.892    10   0.0122 Preprocessor1_Model04\n 9     7 rmse    standard   3315.       10 144.     Preprocessor1_Model05\n10     7 rsq     standard      0.895    10   0.0117 Preprocessor1_Model05\n11     9 rmse    standard   3245.       10 150.     Preprocessor1_Model06\n12     9 rsq     standard      0.898    10   0.0118 Preprocessor1_Model06\n13    10 rmse    standard   3223.       10 148.     Preprocessor1_Model07\n14    10 rsq     standard      0.899    10   0.0123 Preprocessor1_Model07\n15    12 rmse    standard   3182.       10 163.     Preprocessor1_Model08\n16    12 rsq     standard      0.901    10   0.0131 Preprocessor1_Model08\n17    13 rmse    standard   3155.       10 160.     Preprocessor1_Model09\n18    13 rsq     standard      0.903    10   0.0130 Preprocessor1_Model09\n19    15 rmse    standard   3157.       10 160.     Preprocessor1_Model10\n20    15 rsq     standard      0.903    10   0.0130 Preprocessor1_Model10\n\n\nFinding the smallest rmse value and selecting the models best tuning parameter values.\n\nfor_fits |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"rmse\") |&gt;\n  arrange(mean)\n\n# A tibble: 10 × 7\n    mtry .metric .estimator  mean     n std_err .config              \n   &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n 1    13 rmse    standard   3155.    10    160. Preprocessor1_Model09\n 2    15 rmse    standard   3157.    10    160. Preprocessor1_Model10\n 3    12 rmse    standard   3182.    10    163. Preprocessor1_Model08\n 4    10 rmse    standard   3223.    10    148. Preprocessor1_Model07\n 5     9 rmse    standard   3245.    10    150. Preprocessor1_Model06\n 6     7 rmse    standard   3315.    10    144. Preprocessor1_Model05\n 7     6 rmse    standard   3366.    10    143. Preprocessor1_Model04\n 8     4 rmse    standard   3478.    10    166. Preprocessor1_Model03\n 9     3 rmse    standard   3605.    10    183. Preprocessor1_Model02\n10     2 rmse    standard   3876.    10    194. Preprocessor1_Model01\n\nfor_best_params &lt;- select_best(for_fits, metric = \"rmse\")\nfor_best_params\n\n# A tibble: 1 × 2\n   mtry .config              \n  &lt;int&gt; &lt;chr&gt;                \n1    13 Preprocessor1_Model09"
  },
  {
    "objectID": "hw9_code.html#fitting-each-model-on-the-test-set",
    "href": "hw9_code.html#fitting-each-model-on-the-test-set",
    "title": "More Modeling",
    "section": "Fitting each model on the test set",
    "text": "Fitting each model on the test set\nMLR model\n\nmlr_wfl |&gt;\n  last_fit(bike_split) |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard    3531.    Preprocessor1_Model1\n2 rsq     standard       0.881 Preprocessor1_Model1\n\nmlr_final |&gt;\n  predict(bike_test) |&gt;\n  pull() |&gt;\n  rmse_vec(truth = bike_test$bike_count)\n\n[1] 3530.921\n\nalmost_usual_fit &lt;- extract_fit_parsnip(mlr_final)\nusual_fit &lt;- almost_usual_fit$fit\nsummary(usual_fit)\n\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-12000.1  -2551.0    308.9   2549.6  11719.7 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         17793.9      261.1  68.141  &lt; 2e-16 ***\ntemp                -2000.1     4671.5  -0.428 0.668915    \nhumidity            -1957.8     1748.2  -1.120 0.263845    \nwind_speed           -600.4      303.4  -1.979 0.048946 *  \nvis                  -177.6      380.9  -0.466 0.641540    \ndew_point_temp       6604.9     5454.9   1.211 0.227115    \nsolar_radiation      4069.8      491.1   8.288 7.20e-15 ***\nrainfall            -1644.4      340.3  -4.833 2.36e-06 ***\nsnowfall             -351.5      283.7  -1.239 0.216566    \nseasons_Spring      -2126.9      373.0  -5.702 3.35e-08 ***\nseasons_Summer      -1598.2      465.7  -3.432 0.000702 ***\nseasons_Winter      -3649.3      503.7  -7.245 5.41e-12 ***\nholiday_No.Holiday    637.9      267.8   2.382 0.017968 *  \nday_type_Weekend    -1190.1      265.6  -4.481 1.13e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4235 on 249 degrees of freedom\nMultiple R-squared:  0.8274,    Adjusted R-squared:  0.8184 \nF-statistic:  91.8 on 13 and 249 DF,  p-value: &lt; 2.2e-16\n\n\nLASSO Model\n\nlasso_wfl |&gt;\n  finalize_workflow(lowest_rmse) |&gt;\n  last_fit(bike_split) |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard    3551.    Preprocessor1_Model1\n2 rsq     standard       0.879 Preprocessor1_Model1\n\nlasso_final |&gt;\n  predict(bike_test) |&gt;\n  pull() |&gt;\n  rmse_vec(truth = bike_test$bike_count)\n\n[1] 3551.146\n\ntidy(lasso_final)\n\n# A tibble: 14 × 3\n   term               estimate      penalty\n   &lt;chr&gt;                 &lt;dbl&gt;        &lt;dbl&gt;\n 1 (Intercept)         17794.  0.0000000001\n 2 temp                   59.5 0.0000000001\n 3 humidity            -1173.  0.0000000001\n 4 wind_speed           -606.  0.0000000001\n 5 vis                  -124.  0.0000000001\n 6 dew_point_temp       4132.  0.0000000001\n 7 solar_radiation      4054.  0.0000000001\n 8 rainfall            -1691.  0.0000000001\n 9 snowfall             -367.  0.0000000001\n10 seasons_Spring      -2107.  0.0000000001\n11 seasons_Summer      -1548.  0.0000000001\n12 seasons_Winter      -3626.  0.0000000001\n13 holiday_No.Holiday    628.  0.0000000001\n14 day_type_Weekend    -1198.  0.0000000001\n\n\nRegression Tree Model\n\nreg_final &lt;- reg_wfl |&gt;\n  finalize_workflow(reg_best_params)\n\nreg_fit &lt;- reg_final |&gt;\n  last_fit(bike_split)\nreg_fit\n\n# Resampling results\n# Manual resampling \n# A tibble: 1 × 6\n  splits           id               .metrics .notes   .predictions .workflow \n  &lt;list&gt;           &lt;chr&gt;            &lt;list&gt;   &lt;list&gt;   &lt;list&gt;       &lt;list&gt;    \n1 &lt;split [263/90]&gt; train/test split &lt;tibble&gt; &lt;tibble&gt; &lt;tibble&gt;     &lt;workflow&gt;\n\nreg_fit |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard    3362.    Preprocessor1_Model1\n2 rsq     standard       0.886 Preprocessor1_Model1\n\nreg_final_model &lt;- extract_fit_parsnip(reg_fit)\nreg_final_model\n\nparsnip model object\n\nn= 263 \n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n   1) root 263 25867800000 17793.890  \n     2) temp&lt; -0.1789376 109  3319702000  8742.440  \n       4) seasons_Winter&gt;=0.561781 67   234606100  5489.537  \n         8) day_type_Weekend&gt;=0.4646631 16    30455290  3974.625 *\n         9) day_type_Weekend&lt; 0.4646631 51   155911600  5964.804  \n          18) temp&lt; -1.571767 13    13740110  4554.692 *\n          19) temp&gt;=-1.571767 38   107478900  6447.211  \n            38) rainfall&gt;=-0.2868466 7    12980210  5064.857 *\n            39) rainfall&lt; -0.2868466 31    78101950  6759.355  \n              78) vis&gt;=0.2307993 16    46630880  6013.875 *\n              79) vis&lt; 0.2307993 15    13094590  7554.533 *\n       5) seasons_Winter&lt; 0.561781 42  1245196000 13931.600  \n        10) rainfall&gt;=-0.1229395 7    73134670  5814.857 *\n        11) rainfall&lt; -0.1229395 35   618657300 15554.940  \n          22) seasons_Spring&gt;=0.561781 15    56678290 12029.930 *\n          23) seasons_Spring&lt; 0.561781 20   235804600 18198.700  \n            46) solar_radiation&lt; -0.341072 13    94125050 16452.690 *\n            47) solar_radiation&gt;=-0.341072 7    28447930 21441.290 *\n     3) temp&gt;=-0.1789376 154  7297102000 24200.440  \n       6) solar_radiation&lt; -0.9003079 14   395825800  9946.786 *\n       7) solar_radiation&gt;=-0.9003079 140  3772509000 25625.810  \n        14) rainfall&gt;=-0.1325811 22   483428900 20834.950  \n          28) day_type_Weekend&gt;=0.4646631 7    23334590 17453.570 *\n          29) day_type_Weekend&lt; 0.4646631 15   342707800 22412.930 *\n        15) rainfall&lt; -0.1325811 118  2689987000 26519.020  \n          30) temp&lt; 0.2194469 28   446675200 22826.070  \n            60) vis&lt; -0.2589066 12   137828600 19601.670 *\n            61) vis&gt;=-0.2589066 16    90514110 25244.380 *\n          31) temp&gt;=0.2194469 90  1742651000 27667.930  \n            62) temp&gt;=1.476865 13    76335840 21326.540 *\n            63) temp&lt; 1.476865 77  1055282000 28738.560  \n             126) temp&lt; 0.5498581 19   276567200 25860.260 *\n             127) temp&gt;=0.5498581 58   569743700 29681.450  \n               254) temp&gt;=1.255818 12    28446430 26163.920 *\n               255) temp&lt; 1.255818 46   354088000 30599.070  \n                 510) seasons_Summer&lt; 0.5391721 21    84198940 29025.190  \n                  1020) solar_radiation&lt; 0.5190893 7    31381440 27493.290 *\n                  1021) solar_radiation&gt;=0.5190893 14    28176810 29791.140 *\n                 511) seasons_Summer&gt;=0.5391721 25   174174600 31921.120  \n                  1022) temp&gt;=1.020676 7    35922010 29246.710 *\n                  1023) temp&lt; 1.020676 18    68714910 32961.170 *\n\n# creating the regression tree plot\nreg_final_model %&gt;%\n  extract_fit_engine() %&gt;%\n  rpart.plot::rpart.plot(roundint = FALSE)\n\n\n\n\n\n\n\n\nBagged Tree Model\n\nbag_final &lt;- bag_wfl |&gt;\n  finalize_workflow(bag_best_params)\n\nbag_fit &lt;- bag_final |&gt;\n  last_fit(bike_split)\nbag_fit\n\n# Resampling results\n# Manual resampling \n# A tibble: 1 × 6\n  splits           id               .metrics .notes   .predictions .workflow \n  &lt;list&gt;           &lt;chr&gt;            &lt;list&gt;   &lt;list&gt;   &lt;list&gt;       &lt;list&gt;    \n1 &lt;split [263/90]&gt; train/test split &lt;tibble&gt; &lt;tibble&gt; &lt;tibble&gt;     &lt;workflow&gt;\n\nbag_fit |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard    2842.    Preprocessor1_Model1\n2 rsq     standard       0.918 Preprocessor1_Model1\n\nbag_final_model &lt;- extract_fit_parsnip(bag_fit)\nbag_final_model\n\nparsnip model object\n\nBagged CART (regression with 11 members)\n\nVariable importance scores include:\n\n# A tibble: 13 × 4\n   term                      value  std.error  used\n   &lt;chr&gt;                     &lt;dbl&gt;      &lt;dbl&gt; &lt;int&gt;\n 1 temp               19107924851. 413999623.    11\n 2 dew_point_temp     15611279409. 517193627.    11\n 3 solar_radiation    12031959365. 666955578.    11\n 4 seasons_Winter     11603092397. 407815254.    11\n 5 humidity            7379340608. 392943995.    11\n 6 seasons_Summer      2545094820. 908650048.    11\n 7 rainfall            2481673581. 337621524.    11\n 8 wind_speed          1625480677. 490377758.    11\n 9 snowfall            1603914632. 582519687.    10\n10 seasons_Spring      1316701174. 159080717.    11\n11 vis                 1086893276. 146854535.    11\n12 day_type_Weekend     138553176.  40041001.    10\n13 holiday_No.Holiday    50150500.  25411147.     5\n\n# creating the variable importance plot\nbag_importance_tib &lt;- bag_final_model$fit$imp\n\nbag_importance_tib |&gt;\n  mutate(term = factor(term, levels = rev(term))) |&gt;\n  ggplot(aes(x = term, y = value)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip() +\n  labs(\n    title = \"Variable Importance - Bagged Tree Model\",\n    x = \"Variables\",\n    y = \"Importance\"\n  )\n\n\n\n\n\n\n\n\nRandom Forest Model\n\nfor_final &lt;- for_wfl |&gt;\n  finalize_workflow(for_best_params)\n\nfor_fit &lt;- for_final |&gt;\n  last_fit(bike_split)\nfor_fit\n\n# Resampling results\n# Manual resampling \n# A tibble: 1 × 6\n  splits           id               .metrics .notes   .predictions .workflow \n  &lt;list&gt;           &lt;chr&gt;            &lt;list&gt;   &lt;list&gt;   &lt;list&gt;       &lt;list&gt;    \n1 &lt;split [263/90]&gt; train/test split &lt;tibble&gt; &lt;tibble&gt; &lt;tibble&gt;     &lt;workflow&gt;\n\nfor_fit |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard    2862.    Preprocessor1_Model1\n2 rsq     standard       0.918 Preprocessor1_Model1\n\nfor_final_model &lt;- extract_fit_parsnip(for_fit)\nfor_final_model\n\nparsnip model object\n\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, mtry = min_cols(~13L,      x), min.node.size = min_rows(~20, x), importance = ~\"impurity\",      num.threads = 1, verbose = FALSE, seed = sample.int(10^5,          1)) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      263 \nNumber of independent variables:  13 \nMtry:                             13 \nTarget node size:                 20 \nVariable importance mode:         impurity \nSplitrule:                        variance \nOOB prediction error (MSE):       9016521 \nR squared (OOB):                  0.9086769 \n\n# creating the variable importance plot\nfor_importance &lt;- for_final_model$fit$variable.importance\n\nfor_importance_tib &lt;- tibble(\n  term = names(for_importance),\n  value = for_importance\n)\n\nfor_importance_tib |&gt;\n  mutate(term = factor(term, levels = rev(term))) |&gt; \n  ggplot(aes(x = term, y = value)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip() +\n  labs(\n    title = \"Variable Importance - Random Forest Model\",\n    x = \"Variables\",\n    y = \"Importance\"\n  )\n\n\n\n\n\n\n\n\nAfter comparing the final models on the test set, the random forest model is the best to use. This model will be the one fit to the entire bike data set.\n\nfull_data_fit &lt;- for_final |&gt; \n  fit(bike_data)\nfull_data_fit \n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_date()\n• step_mutate()\n• step_rm()\n• step_dummy()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, mtry = min_cols(~13L,      x), min.node.size = min_rows(~20, x), importance = ~\"impurity\",      num.threads = 1, verbose = FALSE, seed = sample.int(10^5,          1)) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      353 \nNumber of independent variables:  13 \nMtry:                             13 \nTarget node size:                 20 \nVariable importance mode:         impurity \nSplitrule:                        variance \nOOB prediction error (MSE):       8292761 \nR squared (OOB):                  0.9160203 \n\nfull_data_model &lt;- extract_fit_parsnip(full_data_fit)\nfull_data_model\n\nparsnip model object\n\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, mtry = min_cols(~13L,      x), min.node.size = min_rows(~20, x), importance = ~\"impurity\",      num.threads = 1, verbose = FALSE, seed = sample.int(10^5,          1)) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      353 \nNumber of independent variables:  13 \nMtry:                             13 \nTarget node size:                 20 \nVariable importance mode:         impurity \nSplitrule:                        variance \nOOB prediction error (MSE):       8292761 \nR squared (OOB):                  0.9160203"
  }
]